{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7RQAoxDrDDz",
        "outputId": "6f632c02-3d26-45f4-d91f-be478e80a469"
      },
      "source": [
        "!pip3 install wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=e520d831e35b11d2e48a19b981485fb097f81578cc2a0f11e5167c5cd2109207\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdyvBQ11eIpa"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup \n",
        "import string\n",
        "import sys\n",
        "sys.setrecursionlimit(1500) # Increase recursion limit\n",
        "from collections import Counter\n",
        "import nltk \n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhrxYxw_rAnW"
      },
      "source": [
        "import pandas as pd\n",
        "import wikipedia\n",
        "wikipedia.set_lang(\"ta\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4YgRnzZ7jqB"
      },
      "source": [
        "people_df=pd.read_csv()#path to the CSV file"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAeydT6BxGwY"
      },
      "source": [
        "names=[] #Names of People"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t04sn9QcLVz"
      },
      "source": [
        "def get_name(string):\n",
        "    \n",
        "    source = requests.get(f\"https://ta.wikipedia.org/wiki/\" + string) # Get access to the article\n",
        "    soup = BeautifulSoup(source.text, 'html.parser')\n",
        "    name = soup.find('h1').text\n",
        "    return name"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JNOfd30c5fd"
      },
      "source": [
        "def get_values(string):\n",
        "    birthdate = None\n",
        "    source = requests.get(f\"https://ta.wikipedia.org/wiki/\" + string) # Get access to the article\n",
        "    soup = BeautifulSoup(source.text, 'html.parser')\n",
        "    if(soup.find('table',attrs={'class':'infobox vcard'})==None):\n",
        "      table_detail= soup.find('table',attrs={'class':'infobox'})\n",
        "    else:\n",
        "      table_detail= soup.find('table',attrs={'class':'infobox vcard'})\n",
        "    table_row_list = table_detail.findAll('tr')\n",
        "    for tr in table_row_list:\n",
        "        try:\n",
        "            if ((tr.th.text).strip() == 'பிறப்பு'):\n",
        "                if tr.td.findAll('span', {'class': 'bday'}):\n",
        "                    birthdate = tr.td.find('span', {'class': 'bday'}).text\n",
        "        except AttributeError or TypeError:\n",
        "            continue\n",
        "    df=pd.read_html(str(table_detail))\n",
        "    list1=df[0].values.tolist()\n",
        "    del list1[0]\n",
        "    dict_values = {}\n",
        "    for elem in list1:\n",
        "      if elem[0] in dict_values:\n",
        "        dict_values[elem[0]].append(elem[1])\n",
        "      else:\n",
        "        dict_values[elem[0]] = [elem[1]]\n",
        "    dict_values['birthdate']=birthdate\n",
        "    return dict_values"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX31NdEStU4k"
      },
      "source": [
        "def get_content(string):\n",
        "  wiki = wikipedia.page(string)\n",
        "  text = wiki.content\n",
        "  text = re.sub(r'==.*?==+', '', text)\n",
        "  text = text.replace('\\n', '.')\n",
        "  text = re.sub(r'\\.+', \".\", text)\n",
        "  text = re.sub(r\"[\\([{'?\\})\\]]\", \"\", text)\n",
        "  #text = re.sub(\"[\\a-zA-Z]+\", \" \",text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOGojERbEN1C"
      },
      "source": [
        "get_content('மில்ட்டன் பிரீட்மன்')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CgPIckE0Jnb"
      },
      "source": [
        "def get_summary(string):\n",
        "  summary=wikipedia.summary(string)\n",
        "  summary = re.sub(r\"^$.\\n\" ,\"\\n\",summary)\n",
        "  summary = summary.replace(\"\\'s\",\"'s\")\n",
        "  summary = summary.replace('\\xa0','')\n",
        "  summary = re.sub(r'\\n+', \"\\n\", summary)\n",
        "  summary = re.sub(r\"[\\([{'?\\})\\]]\", \"\", summary)\n",
        "  #text = re.sub(\"[\\a-zA-Z]+\", \" \",summary)\n",
        "\n",
        "  return summary"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTV6sj9ndSlL"
      },
      "source": [
        "get_summary('மில்ட்டன் பிரீட்மன்')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00BOEScS7zXx"
      },
      "source": [
        "for name in names1:\n",
        "  if name not in people_df['name'].unique():\n",
        "    name=get_name(name)\n",
        "    dict_values= get_values(name)\n",
        "\n",
        "    if('பிறப்பு' in dict_values):\n",
        "        born_year = dict_values['birthdate']\n",
        "\n",
        "    if('துறை' in dict_values):  \n",
        "      area_work=','.join(dict_values['துறை'][0].split(' '))\n",
        "      area_work = re.sub(r'\\,+', \",\", area_work)\n",
        "    else:\n",
        "      area_work=\" \"\n",
        "\n",
        "    if('அறியப்பட்டது' in dict_values):\n",
        "      known_for = ','.join(dict_values['அறியப்பட்டது'][0].split(' '))\n",
        "      known_for = re.sub(r'\\,+', \",\", known_for)\n",
        "    elif ('அறியப்படுவது'in dict_values):\n",
        "      known_for = ','.join(dict_values['அறியப்படுவது'][0].split(' '))\n",
        "      known_for = re.sub(r'\\,+', \",\", known_for)\n",
        "    elif ('பங்களிப்புகள்'in dict_values):\n",
        "      known_for = ','.join(dict_values['பங்களிப்புகள்'][0].split(' '))\n",
        "      known_for = re.sub(r'\\,+', \",\", known_for)  \n",
        "    else:\n",
        "      known_for=' '\n",
        "\n",
        "    if('பரிசுகள்' in dict_values):\n",
        "      awards=','.join(dict_values['பரிசுகள்'][0].split(')'))\n",
        "      awards=awards.replace('(', \"\")\n",
        "    elif('விருதுகள்' in dict_values):\n",
        "      awards=','.join(dict_values['விருதுகள்'][0].split(')'))\n",
        "      awards=awards.replace('(', \"\")\n",
        "    else:\n",
        "      awards= \" \"\n",
        "    count=55\n",
        "    content= get_content(name)\n",
        "    summary= get_summary(name)\n",
        "    url_page ='https://ta.wikipedia.org/wiki/'+ '_'.join(name.split(' '))\n",
        "    series_obj=pd.Series([count,name,born_year,area_work,known_for, awards, summary, content,url_page],index=people_df.columns)\n",
        "    count+=1\n",
        "    people_df = people_df.append(  series_obj,\n",
        "                        ignore_index=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYU0mAjBAOSV"
      },
      "source": [
        "people_df.to_csv('famous_people.csv')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqoFDVuAYoE"
      },
      "source": [
        "!cp famous_people.csv --path_to_save"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}